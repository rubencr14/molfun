"""
Diffusion-based structure module.

Generates 3D coordinates through iterative denoising, inspired by
RFdiffusion and AlphaFold3's diffusion head. Instead of deterministic
refinement (IPA), this module learns to denoise Gaussian-corrupted
coordinates conditioned on single/pair representations.

This is a research scaffold â€” not a full reimplementation of RFdiffusion.
The architecture is intentionally simple so researchers can experiment
with different noise schedules, conditioning mechanisms, and
network architectures.
"""

from __future__ import annotations
from typing import Optional
import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from molfun.modules.structure_module.base import (
    BaseStructureModule,
    StructureModuleOutput,
    STRUCTURE_MODULE_REGISTRY,
)


@STRUCTURE_MODULE_REGISTRY.register("diffusion")
class DiffusionStructureModule(BaseStructureModule):
    """
    Denoising diffusion structure prediction.

    During training, coordinates are noised and the module learns to
    predict the clean coordinates (x0-prediction). During inference,
    coordinates are generated by iteratively denoising from pure noise.

    Args:
        d_single: Single representation dimension.
        d_pair: Pair representation dimension.
        d_model: Internal hidden dimension.
        n_layers: Number of denoising network layers.
        n_steps: Diffusion timesteps for inference.
        noise_schedule: "linear" or "cosine".
    """

    def __init__(
        self,
        d_single: int = 384,
        d_pair: int = 128,
        d_model: int = 256,
        n_layers: int = 4,
        n_steps: int = 100,
        noise_schedule: str = "cosine",
        **kwargs,
    ):
        super().__init__()
        self._d_single = d_single
        self._d_pair = d_pair
        self.n_steps = n_steps

        self.single_proj = nn.Linear(d_single, d_model)
        self.pair_proj = nn.Linear(d_pair, d_model)
        self.coord_embed = nn.Linear(3, d_model)
        self.time_embed = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.SiLU(),
            nn.Linear(d_model, d_model),
        )

        self.layers = nn.ModuleList([
            _DenoisingLayer(d_model, n_heads=8, dropout=0.1)
            for _ in range(n_layers)
        ])

        self.out_proj = nn.Linear(d_model, 3)
        self.norm = nn.LayerNorm(d_model)

        alphas = self._build_schedule(n_steps, noise_schedule)
        self.register_buffer("alphas_cumprod", alphas)

    @staticmethod
    def _build_schedule(n_steps: int, name: str) -> torch.Tensor:
        if name == "cosine":
            t = torch.linspace(0, 1, n_steps + 1)
            alphas = torch.cos((t + 0.008) / 1.008 * math.pi / 2) ** 2
            alphas = alphas / alphas[0]
            return alphas[1:]
        # linear
        betas = torch.linspace(1e-4, 0.02, n_steps)
        return torch.cumprod(1.0 - betas, dim=0)

    def _timestep_embedding(self, t: torch.Tensor, d: int) -> torch.Tensor:
        """Sinusoidal timestep embedding."""
        half = d // 2
        freqs = torch.exp(-math.log(10000) * torch.arange(half, device=t.device) / half)
        args = t.unsqueeze(-1).float() * freqs.unsqueeze(0)
        return torch.cat([torch.sin(args), torch.cos(args)], dim=-1)

    def forward(
        self,
        single: torch.Tensor,
        pair: torch.Tensor,
        aatype: Optional[torch.Tensor] = None,
        mask: Optional[torch.Tensor] = None,
    ) -> StructureModuleOutput:
        B, L, _ = single.shape

        if self.training:
            return self._forward_train(single, pair, mask)
        return self._forward_inference(single, pair, mask)

    def _forward_train(
        self, single: torch.Tensor, pair: torch.Tensor,
        mask: Optional[torch.Tensor],
    ) -> StructureModuleOutput:
        """
        Training: noise GT coords and predict x0.

        NOTE: In a real training loop, GT coords would come from the batch.
        Here we generate dummy targets for the scaffold.
        """
        B, L, _ = single.shape
        device = single.device

        x0 = torch.randn(B, L, 3, device=device)

        t = torch.randint(0, self.n_steps, (B,), device=device)
        alpha_t = self.alphas_cumprod[t].view(B, 1, 1)

        noise = torch.randn_like(x0)
        x_noisy = alpha_t.sqrt() * x0 + (1 - alpha_t).sqrt() * noise

        x_pred = self._denoise(x_noisy, single, pair, t, mask)

        return StructureModuleOutput(
            positions=x_pred,
            extra={"x0": x0, "noise": noise, "t": t},
        )

    @torch.no_grad()
    def _forward_inference(
        self, single: torch.Tensor, pair: torch.Tensor,
        mask: Optional[torch.Tensor],
    ) -> StructureModuleOutput:
        """Inference: iterative denoising from pure noise."""
        B, L, _ = single.shape
        device = single.device

        x = torch.randn(B, L, 3, device=device)

        for step in reversed(range(self.n_steps)):
            t = torch.full((B,), step, device=device, dtype=torch.long)
            x_pred = self._denoise(x, single, pair, t, mask)

            alpha_t = self.alphas_cumprod[step]
            alpha_prev = self.alphas_cumprod[step - 1] if step > 0 else torch.tensor(1.0, device=device)

            # DDPM update
            beta_t = 1.0 - alpha_t / alpha_prev
            x = (1.0 / (1.0 - beta_t).sqrt()) * (
                x - beta_t / (1.0 - alpha_t).sqrt() * (x - alpha_t.sqrt() * x_pred)
            )

            if step > 0:
                x = x + beta_t.sqrt() * torch.randn_like(x)

        return StructureModuleOutput(positions=x)

    def _denoise(
        self,
        x_noisy: torch.Tensor,
        single: torch.Tensor,
        pair: torch.Tensor,
        t: torch.Tensor,
        mask: Optional[torch.Tensor],
    ) -> torch.Tensor:
        """Core denoising network."""
        B, L, _ = single.shape
        d = self.single_proj.out_features

        h = self.single_proj(single) + self.coord_embed(x_noisy)

        t_emb = self._timestep_embedding(t, d)
        h = h + self.time_embed(t_emb).unsqueeze(1)

        pair_h = self.pair_proj(pair.mean(dim=-2))  # [B, L, d_model]
        h = h + pair_h

        for layer in self.layers:
            h = layer(h, mask=mask)

        return self.out_proj(self.norm(h))

    @property
    def d_single(self) -> int:
        return self._d_single

    @property
    def d_pair(self) -> int:
        return self._d_pair


class _DenoisingLayer(nn.Module):
    """Transformer layer for the denoising network."""

    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):
        super().__init__()
        self.norm1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.norm2 = nn.LayerNorm(d_model)
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 4, d_model),
            nn.Dropout(dropout),
        )

    def forward(
        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None,
    ) -> torch.Tensor:
        h = self.norm1(x)
        key_padding_mask = ~mask.bool() if mask is not None else None
        h, _ = self.attn(h, h, h, key_padding_mask=key_padding_mask)
        x = x + h
        x = x + self.ff(self.norm2(x))
        return x
